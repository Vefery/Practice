{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from torch.distributions.normal import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device(\"cuda\"))\n",
    "dim = 128\n",
    "memory_length = 64\n",
    "lstm_layers = 1\n",
    "critic_lr = 3e-4\n",
    "actor_lr = critic_lr / 3.0\n",
    "policy_noise = 0.2\n",
    "noise_clip = 0.5\n",
    "gamma=0.99\n",
    "tau=0.005\n",
    "max_size = 1000000\n",
    "batch_size = 256\n",
    "total_plays = 1000\n",
    "num_epochs = 1\n",
    "N = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#unity_env = UnityEnvironment(\"D:\\Practice\\SentisInfrence\\Build\\SentisInfrence.exe\", no_graphics=True,)\n",
    "#env = UnityToGymWrapper(unity_env)\n",
    "\n",
    "env = gym.make(\"InvertedPendulum-v5\")\n",
    "\n",
    "max_action=env.action_space.high\n",
    "obs_dim = env.observation_space.shape\n",
    "n_actions=env.action_space.shape[-1]\n",
    "print(obs_dim)\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.done_memory = np.zeros((self.mem_size), dtype=bool)\n",
    "        self.hidden_cr1 = (torch.zeros((1, dim), dtype=torch.float).to(device), torch.zeros((1, dim), dtype=torch.float).to(device))\n",
    "        self.hidden_cr2 = (torch.zeros((1, dim), dtype=torch.float).to(device), torch.zeros((1, dim), dtype=torch.float).to(device))\n",
    "        self.hidden_actor = (torch.zeros((1, dim), dtype=torch.float).to(device), torch.zeros((1, dim), dtype=torch.float).to(device))\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, dones, hidden_actor, hidden_critic1, hidden_critic2):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.done_memory[index] = dones\n",
    "        if hidden_actor is not None and hidden_critic1 is not None and hidden_critic2 is not None:\n",
    "            self.hidden_cr1 = (hidden_critic1[0].detach(), hidden_critic1[0].detach())\n",
    "            self.hidden_cr2 = (hidden_critic2[0].detach(), hidden_critic2[0].detach())\n",
    "            self.hidden_actor = (hidden_actor[0].detach(), hidden_actor[0].detach())\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "    \n",
    "    def sample_history_sequence(self, history_length):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        if max_mem <= history_length:\n",
    "            hist_part1 = torch.zeros(1, n_actions * memory_length)\n",
    "            hist_part2 = torch.zeros(1, obs_dim[0] * memory_length)\n",
    "        else:\n",
    "            hist_states = np.zeros([1, history_length, obs_dim[0]])\n",
    "            hist_actions = np.zeros([1, history_length, n_actions])\n",
    "\n",
    "            id = max_mem - 1\n",
    "            hist_start_id = id - history_length\n",
    "            if hist_start_id < 0:\n",
    "                hist_start_id = 0\n",
    "            # If exist done before the last experience (not including the done in id), start from the index next to the done.\n",
    "            if len(np.where(self.done_memory[hist_start_id:id] == 1)[0]) != 0:\n",
    "                hist_start_id = hist_start_id + (np.where(self.done_memory[hist_start_id:id] == True)[0][-1]) + 1\n",
    "            hist_seg_len = id - hist_start_id\n",
    "            hist_states[0, :hist_seg_len, :] = self.state_memory[hist_start_id:id]\n",
    "            hist_actions[0, :hist_seg_len, :] = self.action_memory[hist_start_id:id]\n",
    "\n",
    "            hist_part1 = torch.tensor(hist_actions, dtype=torch.float).reshape(1, -1)\n",
    "            hist_part2 = torch.tensor(hist_states, dtype=torch.float).reshape(1, -1)\n",
    "\n",
    "        dictionary = dict(history_actions=hist_part1,\n",
    "                          history_obs=hist_part2)\n",
    "        return dictionary\n",
    "\n",
    "    def sample_buffer_history(self, batch_size, history_length):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.randint(history_length, max_mem, batch_size)\n",
    "\n",
    "        if history_length == 0:\n",
    "                hist_states = np.zeros([batch_size, 1, obs_dim[0]])\n",
    "                hist_actions = np.zeros([batch_size, 1, n_actions])\n",
    "                hist_states_len = np.zeros(batch_size)\n",
    "        else:\n",
    "            hist_states = np.zeros([batch_size, history_length, obs_dim[0]])\n",
    "            hist_actions = np.zeros([batch_size, history_length, n_actions])\n",
    "            hist_states_len = history_length * np.ones(batch_size)\n",
    "\n",
    "            for i, id, in enumerate(batch):\n",
    "                hist_start_id = id - history_length\n",
    "                if hist_start_id < 0:\n",
    "                    hist_start_id = 0\n",
    "                # If exist done before the last experience (not including the done in id), start from the index next to the done.\n",
    "                if len(np.where(self.done_memory[hist_start_id:id] == 1)[0]) != 0:\n",
    "                    hist_start_id = hist_start_id + (np.where(self.done_memory[hist_start_id:id] == True)[0][-1]) + 1\n",
    "                hist_seg_len = id - hist_start_id\n",
    "                hist_states_len[i] = hist_seg_len\n",
    "                hist_states[i, :hist_seg_len, :] = self.state_memory[hist_start_id:id]\n",
    "                hist_actions[i, :hist_seg_len, :] = self.action_memory[hist_start_id:id]\n",
    "\n",
    "        hist_part1 = torch.tensor(hist_actions, dtype=torch.float).reshape(batch_size, -1)\n",
    "        hist_part2 = torch.tensor(hist_states, dtype=torch.float).reshape(batch_size, -1)\n",
    "\n",
    "        if batch_size <= max_mem:\n",
    "            dictionary = dict(states=self.state_memory[batch],\n",
    "                        states_=self.new_state_memory[batch],\n",
    "                        actions=self.action_memory[batch],\n",
    "                        rewards=self.reward_memory[batch],\n",
    "                        dones=self.done_memory[batch],\n",
    "                        history_actions=hist_part1,\n",
    "                        history_obs=hist_part2,\n",
    "                        hidden_critic1=self.hidden_cr1,\n",
    "                        hidden_critic2=self.hidden_cr2,\n",
    "                        hidden_actor=self.hidden_actor)\n",
    "        else:\n",
    "            dictionary = dict(history_actions=hist_part1,\n",
    "                              history_obs=hist_part2,\n",
    "                              hidden_critic1=self.hidden_cr1,\n",
    "                              hidden_critic2=self.hidden_cr2,\n",
    "                              hidden_actor=self.hidden_actor)\n",
    "        \n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HistoryNetwork, self).__init__()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(nn.Linear((obs_dim[-1] + n_actions) * memory_length, dim),\n",
    "                                       nn.ReLU())\n",
    "        self.lstm_layers = nn.LSTM(dim, dim, num_layers=lstm_layers, batch_first=True)\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, history_actions, history_obs, hidden: tuple[torch.Tensor, torch.Tensor]):\n",
    "        history = torch.cat((history_actions, history_obs), dim=-1)\n",
    "\n",
    "        x = self.fc_layers(history)\n",
    "        if hidden is not None:\n",
    "            out, hidden_ = self.lstm_layers(x)\n",
    "        else:\n",
    "            out, hidden_ = self.lstm_layers(x)\n",
    "\n",
    "        return out, hidden_\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.critic_me = HistoryNetwork()\n",
    "\n",
    "        self.critic_cf = nn.Sequential(\n",
    "            nn.Linear(obs_dim[-1] + n_actions, dim),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.critic_pi = nn.Sequential(nn.Linear(2 * dim, dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(dim, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=critic_lr)\n",
    "        #self.warmup_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=critic_lr, last_epoch=1000)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state, action, history_actions, history_obs, hidden: tuple[torch.Tensor, torch.Tensor]):\n",
    "        me, hidden_ = self.critic_me(history_actions, history_obs, hidden)\n",
    "        cf = self.critic_cf(torch.cat([state, action], dim=1))\n",
    "\n",
    "        pi = self.critic_pi(torch.cat([me, cf], dim=1))\n",
    "        return pi, hidden_\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.actor_me = HistoryNetwork()\n",
    "        self.actor_cf = nn.Sequential(\n",
    "            nn.Linear(obs_dim[-1], dim),\n",
    "            nn.ReLU())\n",
    "        self.actor_pi = nn.Sequential(nn.Linear(2 * dim, dim),\n",
    "                                      nn.ReLU())\n",
    "        self.loc = nn.Sequential(nn.Linear(dim, n_actions),\n",
    "                                    nn.Tanh())\n",
    "        self.scale = nn.Sequential(nn.Linear(dim, n_actions),\n",
    "                                    nn.Tanh())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=actor_lr)\n",
    "        #self.warmup_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=actor_lr, last_epoch=1000)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state, history_actions, history_obs, hidden: tuple[torch.Tensor, torch.Tensor]):\n",
    "        me, hidden_ = self.actor_me(history_actions, history_obs, hidden)\n",
    "        cf = self.actor_cf(state)\n",
    "        pi = self.actor_pi(torch.cat([me, cf], dim=-1))\n",
    "\n",
    "        loc = self.loc(pi)\n",
    "        scale_log = self.scale(pi)\n",
    "        scale_log = torch.clamp(scale_log, min=-20, max=2)\n",
    "\n",
    "        return loc, scale_log, hidden_\n",
    "\n",
    "    def sample_normal(self, state, history_actions, history_obs, hidden=None):\n",
    "        loc, scale_log, hidden_ = self.forward(state, history_actions, history_obs, hidden)\n",
    "        scale = scale_log.exp()\n",
    "        dist = Normal(loc, scale)\n",
    "\n",
    "        sample = dist.rsample()\n",
    "\n",
    "        action = torch.tanh(sample)*torch.tensor(env.action_space.high).to(device)\n",
    "\n",
    "        return action, hidden_\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer(max_size, obs_dim, n_actions)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "        self.actor = ActorNetwork()\n",
    "        self.actor_target = ActorNetwork()\n",
    "        self.critic_1 = CriticNetwork()\n",
    "        self.critic_2 = CriticNetwork()\n",
    "        self.critic_1_target = CriticNetwork()\n",
    "        self.critic_2_target = CriticNetwork()\n",
    "\n",
    "        self.actor.apply(init_weights)\n",
    "        self.critic_1.apply(init_weights)\n",
    "        self.critic_2.apply(init_weights)\n",
    "\n",
    "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_target.load_state_dict(self.actor_target.state_dict())\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        dic = self.memory.sample_history_sequence(memory_length)\n",
    "        history_actions = dic[\"history_actions\"].to(device)\n",
    "        history_obs = dic[\"history_obs\"].to(device)\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(device)\n",
    "        actions, _ = self.actor.sample_normal(state, history_actions, history_obs)\n",
    "\n",
    "        return actions.cpu().detach().numpy()[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, dones, hidden_critic1, hidden_critic2, hidden_actor):\n",
    "        self.memory.store_transition(state, action, reward, new_state, dones, hidden_critic1, hidden_critic2, hidden_actor)\n",
    "\n",
    "    def gradient_step(self):\n",
    "        if self.memory.mem_cntr < batch_size:\n",
    "\n",
    "            return None, None, None\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            dct = self.memory.sample_buffer_history(batch_size, memory_length)\n",
    "\n",
    "            history_actions = dct[\"history_actions\"].detach().to(device)\n",
    "            history_obs = dct[\"history_obs\"].detach().to(device)\n",
    "            hidden_critic1 = dct[\"hidden_critic1\"]\n",
    "            hidden_critic2 = dct[\"hidden_critic2\"]\n",
    "            hidden_actor = dct[\"hidden_actor\"]\n",
    "            reward = torch.tensor(dct[\"rewards\"], dtype=torch.float).to(device)\n",
    "            state_ = torch.tensor(dct[\"states_\"], dtype=torch.float).to(device)\n",
    "            state = torch.tensor(dct[\"states\"], dtype=torch.float).to(device)\n",
    "            actions = torch.tensor(dct[\"actions\"], dtype=torch.float).to(device)\n",
    "            dones = torch.tensor(dct[\"dones\"], dtype=torch.bool).to(device)\n",
    "\n",
    "            # Critics gradient step\n",
    "            actions_, _ = self.actor_target.sample_normal(state_, history_actions, history_obs, hidden_actor)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                noise = (torch.randn_like(actions) * policy_noise).clamp(-noise_clip, noise_clip).to(device)\n",
    "                actions_ = (actions_ + noise).clamp(torch.tensor(env.action_space.low, device=device), torch.tensor(env.action_space.high, device=device))\n",
    "                q1_target_value, _ = self.critic_1_target.forward(state_, actions_, history_actions, history_obs, hidden_critic1)\n",
    "                q2_target_value, _ = self.critic_2_target.forward(state_, actions_, history_actions, history_obs, hidden_critic2)\n",
    "                q_target_value = torch.min(q2_target_value, q1_target_value)\n",
    "                q_hat = reward.view(batch_size, -1) + gamma * ~(dones.view(batch_size, -1)) * q_target_value\n",
    "            q1_value, hidden_critic1_ = self.critic_1.forward(state, actions, history_actions, history_obs, hidden_critic1)\n",
    "            q2_value, hidden_critic2_ = self.critic_2.forward(state, actions, history_actions, history_obs, hidden_critic2)\n",
    "            q1_loss = 0.5 * F.mse_loss(q1_value, q_hat)\n",
    "            q2_loss = 0.5 * F.mse_loss(q2_value, q_hat)\n",
    "            \n",
    "            q_loss = q1_loss + q2_loss\n",
    "            self.critic_1.zero_grad()\n",
    "            self.critic_2.zero_grad()\n",
    "            q_loss.backward()\n",
    "            self.critic_1.optimizer.step()\n",
    "            self.critic_2.optimizer.step()\n",
    "\n",
    "            # Policy gradient step\n",
    "            actions_reparam, hidden_actor_ = self.actor.sample_normal(state, history_actions, history_obs, hidden_actor)\n",
    "            q1_value, _ = self.critic_1.forward(state, actions_reparam, history_actions, history_obs, hidden_critic1)\n",
    "            actor_loss = -q1_value.mean()\n",
    "            self.actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "\n",
    "            # Target critic weights update\n",
    "            for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        return hidden_actor_, hidden_critic1_, hidden_critic2_\n",
    "    \n",
    "    def save_model(self):\n",
    "        model_scripted = torch.jit.script(self.actor)\n",
    "        model_scripted.save(f\"models/{env.spec.name}_test\" + \"_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 44/1000 [19:46<7:09:48, 26.98s/it]\n",
      " 71%|███████   | 712/1000 [19:33<11:52,  2.48s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iter_steps \u001b[38;5;241m%\u001b[39m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 43\u001b[0m     hidden_actor, hidden_critic1, hidden_critic2 \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(observation, action, reward, observation_, terminated, hidden_actor, hidden_critic1, hidden_critic2)\n\u001b[0;32m     45\u001b[0m iter_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[37], line 164\u001b[0m, in \u001b[0;36mAgent.gradient_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_2\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    163\u001b[0m q_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_2\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Policy gradient step\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\torch\\optim\\adam.py:579\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    577\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    582\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if \"pbar\" in globals():\n",
    "    pbar.close()\n",
    "pbar = tqdm(total=total_plays)\n",
    "pbar.reset()\n",
    "writer = SummaryWriter(f\"logs/{env.spec.name}_test\" + str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute))\n",
    "\n",
    "writer.add_text(\n",
    "          \"Hyperparameters\",\n",
    "          \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join(\n",
    "               [f\"|Critic lr|{critic_lr}|\",\n",
    "                f\"|Actor lr|{actor_lr}|\",\n",
    "                f\"|Layer dim|{dim}|\",\n",
    "                f\"|LSTM layers|{lstm_layers}|\",\n",
    "                f\"|Memory length|{memory_length}|\",\n",
    "                f\"|Batch size|{batch_size}|\",\n",
    "                f\"|Gamma|{gamma}|\",\n",
    "                f\"|Tau|{tau}|\",\n",
    "                ]\n",
    "          )),\n",
    "          int(str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute)))\n",
    "\n",
    "\n",
    "agent = Agent()\n",
    "best_score = -1000000\n",
    "score_history = []\n",
    "\n",
    "global_step = 0\n",
    "for i in range(total_plays):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    iter_steps = 0\n",
    "    #rooms = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        #if terminated and iter_steps == 400:\n",
    "        #    terminated = False\n",
    "        #rooms = max(rooms, observation_[-1])\n",
    "        score += reward\n",
    "        if iter_steps % N == 0:\n",
    "            hidden_actor, hidden_critic1, hidden_critic2 = agent.gradient_step()\n",
    "        agent.remember(observation, action, reward, observation_, terminated, hidden_actor, hidden_critic1, hidden_critic2)\n",
    "        iter_steps += 1\n",
    "        global_step += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_model()\n",
    "\n",
    "    writer.add_scalar(\"charts/reward\", avg_score, global_step=global_step)\n",
    "    #writer.add_scalar(\"charts/rooms\", rooms, global_step=global_step)\n",
    "    writer.add_scalar(\"charts/step_count\", iter_steps, global_step=global_step)\n",
    "    pbar.update()\n",
    "\n",
    "pbar.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'actor_state_dict': agent.actor.state_dict(),\n",
    "            'critic1_state_dict': agent.critic_1.state_dict(),\n",
    "            'critic2_state_dict': agent.critic_2.state_dict(),\n",
    "            'target_critic1_state_dict': agent.critic_1_target.state_dict(),\n",
    "            'target_critic2_state_dict': agent.critic_2_target.state_dict(),\n",
    "            'actor_optimizer_state_dict': agent.actor.optimizer.state_dict(),\n",
    "            'critic1_optimizer_state_dict': agent.critic_1.optimizer.state_dict(),\n",
    "            'critic2_optimizer_state_dict': agent.critic_2.optimizer.state_dict(),\n",
    "            }, \"models/unity\" + str(i) + \"_steps_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"models/unity999_steps_weights.pt\")\n",
    "agent.actor.load_state_dict(ckpt['actor_state_dict'])\n",
    "agent.actor.optimizer.load_state_dict(ckpt['actor_optimizer_state_dict'])\n",
    "agent.critic_1.load_state_dict(ckpt['critic1_state_dict'])\n",
    "agent.critic_2.load_state_dict(ckpt['critic2_state_dict'])\n",
    "agent.critic_1.optimizer.load_state_dict(ckpt['critic1_optimizer_state_dict'])\n",
    "agent.critic_2.optimizer.load_state_dict(ckpt['critic2_optimizer_state_dict'])\n",
    "agent.critic_1_target.load_state_dict(ckpt['target_critic1_state_dict'])\n",
    "agent.critic_2_target.load_state_dict(ckpt['target_critic2_state_dict'])\n",
    "agent.actor.train()\n",
    "agent.critic_1.train()\n",
    "agent.critic_2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX (Unity format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset() \n",
    "observation = torch.tensor(np.array([observation]), dtype=torch.float).to(device)\n",
    "\n",
    "class WrapperNet(torch.nn.Module):\n",
    "    def __init__(self, actor, max_action):\n",
    "        super(WrapperNet, self).__init__()\n",
    "        self.qnet = actor\n",
    "        self.max_action = max_action\n",
    "    \n",
    "    def forward(self, obs: torch.tensor, history_actions: torch.tensor, history_obs: torch.tensor):\n",
    "        loc, scale_log = self.qnet(obs, history_actions, history_obs)\n",
    "        scale = scale_log.exp()\n",
    "        dist = Normal(loc, scale)\n",
    "\n",
    "        sample = dist.sample()\n",
    "\n",
    "        action = torch.tanh(sample)*torch.tensor(max_action).to(device)\n",
    "\n",
    "        \n",
    "        return action\n",
    "    \n",
    "model = WrapperNet(agent.actor, env.action_space.shape)\n",
    "model.eval()\n",
    "torch.onnx.CheckerError(model)\n",
    "history_actions = torch.zeros(1, n_actions * memory_length).to(device)\n",
    "history_obs = torch.zeros(1, obs_dim[0] * memory_length).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (observation, history_actions, history_obs),\n",
    "    'UnityTest.onnx',\n",
    "    input_names=[\"obs\", \"hist_actions\", \"hist_obs\"],\n",
    "    output_names=[\"continuous_actions\"]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
