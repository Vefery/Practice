{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from torch.distributions.normal import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device(\"cuda\"))\n",
    "dim = 256\n",
    "lr = 3e-4\n",
    "reparam_noise = 1e-6\n",
    "gamma=0.99\n",
    "tau=5e-3\n",
    "max_size=1000000\n",
    "batch_size = 256\n",
    "total_plays = 1000\n",
    "num_epochs = 1\n",
    "N = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unity_env = UnityEnvironment(\"path to .exe\")\n",
    "#env = UnityToGymWrapper(unity_env)\n",
    "\n",
    "env = gym.make(\"HalfCheetah-v4\")\n",
    "\n",
    "max_action=env.action_space.high\n",
    "obs_dim = env.observation_space.shape\n",
    "n_actions=env.action_space.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(obs_dim[-1] + n_actions, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = self.layers(torch.cat([state, action], dim=1))\n",
    "\n",
    "        return q\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(*obs_dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.loc = nn.Linear(dim, n_actions)\n",
    "        self.scale = nn.Linear(dim, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        relu = self.layers(state)\n",
    "\n",
    "        loc = self.loc(relu)\n",
    "        scale_log = self.scale(relu)\n",
    "        scale_log = torch.clamp(scale_log, min=-20, max=2)\n",
    "\n",
    "        return loc, scale_log\n",
    "\n",
    "    def sample_normal(self, state, reparameterize=True):\n",
    "        loc, scale_log = self.forward(state)\n",
    "        scale = scale_log.exp()\n",
    "        dist = Normal(loc, scale)\n",
    "\n",
    "        if reparameterize:\n",
    "            sample = dist.rsample()\n",
    "        else:\n",
    "            sample = dist.sample()\n",
    "\n",
    "        action = torch.tanh(sample)*torch.tensor(env.action_space.high).to(device)\n",
    "        log_probs = dist.log_prob(sample)\n",
    "        log_probs -= torch.log(1 - action.pow(2) + reparam_noise)\n",
    "        log_probs = log_probs.sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_probs\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer(max_size, obs_dim, n_actions)\n",
    "\n",
    "        self.actor = ActorNetwork()\n",
    "        self.critic_1 = CriticNetwork()\n",
    "        self.critic_2 = CriticNetwork()\n",
    "        self.critic_1_target = CriticNetwork()\n",
    "        self.critic_2_target = CriticNetwork()\n",
    "\n",
    "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
    "\n",
    "        self.alpha = 1e-6\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr)\n",
    "        self.target_entropy = -n_actions\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(device)\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "\n",
    "        return actions.cpu().detach().numpy()[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state):\n",
    "        self.memory.store_transition(state, action, reward, new_state)\n",
    "\n",
    "    def gradient_step(self):\n",
    "        if self.memory.mem_cntr < batch_size:\n",
    "            return 0, 0, 0, 0\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            state, actions, reward, state_ = self.memory.sample_buffer(batch_size)\n",
    "\n",
    "            reward = torch.tensor(reward, dtype=torch.float).to(device)\n",
    "            state_ = torch.tensor(state_, dtype=torch.float).to(device)\n",
    "            state = torch.tensor(state, dtype=torch.float).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "\n",
    "            start = time.time()\n",
    "            # Critics gradient step\n",
    "            actions_, log_probs_ = self.actor.sample_normal(state_, reparameterize=False)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q1_target_value = self.critic_1_target.forward(state_, actions_)\n",
    "                q2_target_value = self.critic_2_target.forward(state_, actions_)\n",
    "                q_target_value = torch.min(q2_target_value, q1_target_value)\n",
    "                q_hat = reward.view(batch_size, -1) + gamma * (q_target_value - self.alpha * log_probs_)\n",
    "            q1_value = self.critic_1.forward(state, actions)\n",
    "            q2_value = self.critic_2.forward(state, actions)\n",
    "            q1_loss = 0.5 * F.mse_loss(q1_value, q_hat)\n",
    "            q2_loss = 0.5 * F.mse_loss(q2_value, q_hat)\n",
    "            \n",
    "            q_loss = q1_loss + q2_loss\n",
    "            self.critic_1.zero_grad()\n",
    "            self.critic_2.zero_grad()\n",
    "            q_loss.backward()\n",
    "            self.critic_1.optimizer.step()\n",
    "            self.critic_2.optimizer.step()\n",
    "\n",
    "            end = time.time()\n",
    "            critics_time = end - start\n",
    "\n",
    "            start = time.time()\n",
    "            # Policy gradient step\n",
    "            actions_reparam, log_prob_reparam = self.actor.sample_normal(state, reparameterize=True)\n",
    "            q1_value = self.critic_1.forward(state, actions_reparam)\n",
    "            q2_value = self.critic_2.forward(state, actions_reparam)\n",
    "            q_value = torch.min(q1_value, q2_value)\n",
    "            actor_loss = (self.alpha * log_prob_reparam - q_value).mean()\n",
    "            self.actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "\n",
    "            end = time.time()\n",
    "            actor_time = end - start\n",
    "\n",
    "            start = time.time()\n",
    "            # Alpha gradient step\n",
    "            _, log_probs = self.actor.sample_normal(state, reparameterize=False)\n",
    "            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            self.alpha = self.log_alpha.exp().item()\n",
    "\n",
    "            end = time.time()\n",
    "            alpha_time = end - start\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Target critic weights update\n",
    "            target_critic1_params = self.critic_1_target.named_parameters()\n",
    "            critic1_params = self.critic_1.named_parameters()\n",
    "            target_critic1_state_dict = dict(target_critic1_params)\n",
    "            critic1_state_dict = dict(critic1_params)\n",
    "\n",
    "            for name in target_critic1_state_dict:\n",
    "                target_critic1_state_dict[name] = tau * critic1_state_dict[name].clone() + (1 - tau) * target_critic1_state_dict[name].clone()\n",
    "\n",
    "            self.critic_1_target.load_state_dict(target_critic1_state_dict)\n",
    "\n",
    "            target_critic2_params = self.critic_2_target.named_parameters()\n",
    "            critic2_params = self.critic_2.named_parameters()\n",
    "            target_critic2_state_dict = dict(target_critic2_params)\n",
    "            critic2_state_dict = dict(critic2_params)\n",
    "\n",
    "            for name in target_critic2_state_dict:\n",
    "                target_critic2_state_dict[name] = tau * critic2_state_dict[name].clone() + (1 - tau) * target_critic2_state_dict[name].clone()\n",
    "\n",
    "            self.critic_2_target.load_state_dict(target_critic2_state_dict)\n",
    "\n",
    "            end = time.time()\n",
    "            weight_update_time = end - start\n",
    "        \n",
    "        return critics_time, actor_time, alpha_time, weight_update_time\n",
    "    \n",
    "    def save_model(self):\n",
    "        model_scripted = torch.jit.script(self.actor)\n",
    "        model_scripted.save(\"models/cheetah\" + \"_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=total_plays)\n",
    "pbar.reset()\n",
    "writer = SummaryWriter(\"logs/cheetah\" + str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute))\n",
    "\n",
    "writer.add_text(\n",
    "         \"Hyperparameters\",\n",
    "         \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join(\n",
    "              [f\"|lr|{lr}|\",\n",
    "               f\"|Layer dim|{dim}|\",\n",
    "               f\"|Batch size|{batch_size}|\",\n",
    "               f\"|Gamma|{gamma}|\",\n",
    "               f\"|Tau|{tau}|\",\n",
    "               ]\n",
    "         )),\n",
    "         int(str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute)))\n",
    "\n",
    "agent = Agent()\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "global_step = 0\n",
    "for i in range(total_plays):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        iter_steps = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            score += reward\n",
    "            agent.remember(observation, action, reward, observation_)\n",
    "            critics_time, actor_time, alpha_time, weight_update_time = agent.gradient_step()\n",
    "\n",
    "            writer.add_custom_scalars_multilinechart([\"time_metrics/critic\", \"time_metrics/actor\", \"time_metrics/alpha\", \"time_metrics/weights\"], title=\"Times\")\n",
    "            writer.add_scalar(\"time_metrics/critic\", critics_time, global_step=global_step)\n",
    "            writer.add_scalar(\"time_metrics/actor\", actor_time, global_step=global_step)\n",
    "            writer.add_scalar(\"time_metrics/alpha\", alpha_time, global_step=global_step)\n",
    "            writer.add_scalar(\"time_metrics/weights\", weight_update_time, global_step=global_step)\n",
    "\n",
    "            iter_steps += 1\n",
    "            global_step += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_model()\n",
    "\n",
    "        writer.add_scalar(\"charts/reward\", avg_score, global_step=i)\n",
    "        writer.add_scalar(\"charts/step_count\", iter_steps, global_step=i)\n",
    "        pbar.update()\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1000 [20:26<5:14:38, 20.10s/it]\n"
     ]
    }
   ],
   "source": [
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
