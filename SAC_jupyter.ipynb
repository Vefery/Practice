{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from torch.distributions.normal import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device(\"cuda\"))\n",
    "dim = 256\n",
    "critic_lr = 3e-4\n",
    "actor_lr = critic_lr / 3.0\n",
    "reparam_noise = 1e-6\n",
    "gamma=0.99\n",
    "tau=0.005\n",
    "alpha_start = 1\n",
    "max_size = 1000000\n",
    "batch_size = 512\n",
    "total_plays = 1000\n",
    "num_epochs = 2\n",
    "N = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "(225,)\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "unity_env = UnityEnvironment(\"D:\\Practice\\SentisInfrence\\Build\\SentisInfrence.exe\", no_graphics=False)\n",
    "env = UnityToGymWrapper(unity_env)\n",
    "\n",
    "#env = gym.make(\"InvertedDoublePendulum-v5\")\n",
    "\n",
    "max_action=env.action_space.high\n",
    "obs_dim = env.observation_space.shape\n",
    "n_actions=env.action_space.shape[-1]\n",
    "\n",
    "print(obs_dim)\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.done_memory = np.zeros((self.mem_size), dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, dones):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.done_memory[index] = dones\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.done_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(obs_dim[-1] + n_actions, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=critic_lr)\n",
    "        #self.warmup_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=critic_lr, last_epoch=1000)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = self.layers(torch.cat([state, action], dim=1))\n",
    "\n",
    "        return q\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(*obs_dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.loc = nn.Linear(dim, n_actions)\n",
    "        self.scale = nn.Linear(dim, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=actor_lr)\n",
    "        #self.warmup_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=actor_lr, last_epoch=1000)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        relu = self.layers(state)\n",
    "\n",
    "        loc = self.loc(relu)\n",
    "        scale_log = self.scale(relu)\n",
    "        scale_log = torch.clamp(scale_log, min=-20, max=2)\n",
    "\n",
    "        return loc, scale_log\n",
    "\n",
    "    def sample_normal(self, state, reparameterize=True):\n",
    "        loc, scale_log = self.forward(state)\n",
    "        scale = scale_log.exp()\n",
    "        dist = Normal(loc, scale)\n",
    "\n",
    "        if reparameterize:\n",
    "            sample = dist.rsample()\n",
    "        else:\n",
    "            sample = dist.sample()\n",
    "\n",
    "        action = torch.tanh(sample)*torch.tensor(env.action_space.high).to(device)\n",
    "        log_probs = dist.log_prob(sample)\n",
    "        log_probs = log_probs.sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_probs\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer(max_size, obs_dim, n_actions)\n",
    "\n",
    "        self.actor = ActorNetwork()\n",
    "        self.critic_1 = CriticNetwork()\n",
    "        self.critic_2 = CriticNetwork()\n",
    "        self.critic_1_target = CriticNetwork()\n",
    "        self.critic_2_target = CriticNetwork()\n",
    "\n",
    "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
    "\n",
    "        self.alpha = alpha_start\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=critic_lr)\n",
    "        #self.alpha_warmup_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.alpha_optimizer, T_max=critic_lr, last_epoch=1000)\n",
    "        self.target_entropy = -n_actions\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor(np.array([observation], dtype=float), dtype=torch.float).to(device)\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "\n",
    "        return actions.cpu().detach().numpy()[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, dones):\n",
    "        self.memory.store_transition(state, action, reward, new_state, dones)\n",
    "\n",
    "    def gradient_step(self):\n",
    "        if self.memory.mem_cntr < batch_size:\n",
    "            return\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            state, actions, reward, state_, dones = self.memory.sample_buffer(batch_size)\n",
    "\n",
    "            reward = torch.tensor(reward, dtype=torch.float).to(device)\n",
    "            state_ = torch.tensor(state_, dtype=torch.float).to(device)\n",
    "            state = torch.tensor(state, dtype=torch.float).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.bool).to(device)\n",
    "\n",
    "            # Critics gradient step\n",
    "            actions_, log_probs_ = self.actor.sample_normal(state_, reparameterize=False)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q1_target_value = self.critic_1_target.forward(state_, actions_)\n",
    "                q2_target_value = self.critic_2_target.forward(state_, actions_)\n",
    "                q_target_value = torch.min(q2_target_value, q1_target_value)\n",
    "                q_hat = reward.view(batch_size, -1) + gamma * ~(dones.view(batch_size, -1)) * (q_target_value - self.alpha * log_probs_)\n",
    "            q1_value = self.critic_1.forward(state, actions)\n",
    "            q2_value = self.critic_2.forward(state, actions)\n",
    "            q1_loss = 0.5 * F.mse_loss(q1_value, q_hat)\n",
    "            q2_loss = 0.5 * F.mse_loss(q2_value, q_hat)\n",
    "            \n",
    "            q_loss = q1_loss + q2_loss\n",
    "            self.critic_1.zero_grad()\n",
    "            self.critic_2.zero_grad()\n",
    "            q_loss.backward()\n",
    "            self.critic_1.optimizer.step()\n",
    "            self.critic_2.optimizer.step()\n",
    "\n",
    "            # Policy gradient step\n",
    "            actions_reparam, log_prob_reparam = self.actor.sample_normal(state, reparameterize=True)\n",
    "            q1_value = self.critic_1.forward(state, actions_reparam)\n",
    "            q2_value = self.critic_2.forward(state, actions_reparam)\n",
    "            q_value = torch.min(q1_value, q2_value)\n",
    "            actor_loss = (self.alpha * log_prob_reparam - q_value).mean()\n",
    "            self.actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "\n",
    "            # Alpha gradient step\n",
    "            _, log_probs = self.actor.sample_normal(state, reparameterize=False)\n",
    "            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            self.alpha = self.log_alpha.exp().item()\n",
    "\n",
    "            # Target critic weights update\n",
    "            target_critic1_params = self.critic_1_target.named_parameters()\n",
    "            critic1_params = self.critic_1.named_parameters()\n",
    "            target_critic1_state_dict = dict(target_critic1_params)\n",
    "            critic1_state_dict = dict(critic1_params)\n",
    "\n",
    "            for name in target_critic1_state_dict:\n",
    "                target_critic1_state_dict[name] = tau * target_critic1_state_dict[name].clone() + (1 - tau) * critic1_state_dict[name].clone()\n",
    "\n",
    "            self.critic_1_target.load_state_dict(target_critic1_state_dict)\n",
    "\n",
    "            target_critic2_params = self.critic_2_target.named_parameters()\n",
    "            critic2_params = self.critic_2.named_parameters()\n",
    "            target_critic2_state_dict = dict(target_critic2_params)\n",
    "            critic2_state_dict = dict(critic2_params)\n",
    "\n",
    "            for name in target_critic2_state_dict:\n",
    "                target_critic2_state_dict[name] = tau * target_critic2_state_dict[name].clone() + (1 - tau) * critic2_state_dict[name].clone()\n",
    "\n",
    "            self.critic_2_target.load_state_dict(target_critic2_state_dict)\n",
    "\n",
    "            return\n",
    "    \n",
    "    def save_model(self):\n",
    "        model_scripted = torch.jit.script(self.actor)\n",
    "        model_scripted.save(\"models/unity_test\" + \"_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 80/1000 [37:47<10:46:30, 42.16s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     32\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(observation)\n\u001b[1;32m---> 33\u001b[0m     observation_, reward, terminated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated\n\u001b[0;32m     35\u001b[0m     rooms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(rooms, observation_[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\envs\\unity_gym_env.py:200\u001b[0m, in \u001b[0;36mUnityToGymWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    197\u001b[0m     action_tuple\u001b[38;5;241m.\u001b[39madd_discrete(action)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mset_actions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, action_tuple)\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m decision_step, terminal_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mget_steps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_agents(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(decision_step), \u001b[38;5;28mlen\u001b[39m(terminal_step)))\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\environment.py:348\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    346\u001b[0m step_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_step_input(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicator.exchange\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexchange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py:142\u001b[0m, in \u001b[0;36mRpcCommunicator.exchange\u001b[1;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[0;32m    140\u001b[0m message\u001b[38;5;241m.\u001b[39munity_input\u001b[38;5;241m.\u001b[39mCopyFrom(inputs)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39msend(message)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll_for_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py:106\u001b[0m, in \u001b[0;36mRpcCommunicator.poll_for_timeout\u001b[1;34m(self, poll_callback)\u001b[0m\n\u001b[0;32m    104\u001b[0m callback_timeout_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout_wait \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m<\u001b[39m deadline:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_to_external\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_timeout_wait\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;66;03m# Got an acknowledgment from the connection\u001b[39;00m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m poll_callback:\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;66;03m# Fire the callback - if it detects something wrong, it should raise an exception.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\mlagents\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if \"pbar\" in globals():\n",
    "    pbar.close()\n",
    "pbar = tqdm(total=total_plays)\n",
    "pbar.reset()\n",
    "writer = SummaryWriter(\"logs/unity_test\" + str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute))\n",
    "\n",
    "writer.add_text(\n",
    "          \"Hyperparameters\",\n",
    "          \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join(\n",
    "               [f\"|Critic lr|{critic_lr}|\",\n",
    "                f\"|Actor lr|{actor_lr}|\",\n",
    "                f\"|Layer dim|{dim}|\",\n",
    "                f\"|Batch size|{batch_size}|\",\n",
    "                f\"|Gamma|{gamma}|\",\n",
    "                f\"|Tau|{tau}|\",\n",
    "                ]\n",
    "          )),\n",
    "          int(str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute)))\n",
    "\n",
    "agent = Agent()\n",
    "best_score = -100000\n",
    "score_history = []\n",
    "\n",
    "global_step = 0\n",
    "for i in range(total_plays):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    iter_steps = 0\n",
    "    rooms = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, terminated, _ = env.step(action)\n",
    "        done = terminated\n",
    "        rooms = max(rooms, observation_[-1])\n",
    "        if terminated and iter_steps == 500:\n",
    "            terminated = False\n",
    "        score += reward\n",
    "        agent.remember(observation, action, reward, observation_, terminated)\n",
    "        if iter_steps % N == 0:\n",
    "            agent.gradient_step()\n",
    "        iter_steps += 1\n",
    "        global_step += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_model()\n",
    "\n",
    "    writer.add_scalar(\"charts/reward\", avg_score, global_step=global_step)\n",
    "    writer.add_scalar(\"charts/rooms\", rooms, global_step=global_step)\n",
    "    writer.add_scalar(\"charts/step_count\", iter_steps, global_step=global_step)\n",
    "    pbar.update()\n",
    "\n",
    "pbar.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'actor_state_dict': agent.actor.state_dict(),\n",
    "            'critic1_state_dict': agent.critic_1.state_dict(),\n",
    "            'critic2_state_dict': agent.critic_2.state_dict(),\n",
    "            'target_critic1_state_dict': agent.critic_1_target.state_dict(),\n",
    "            'target_critic2_state_dict': agent.critic_2_target.state_dict(),\n",
    "            'actor_optimizer_state_dict': agent.actor.optimizer.state_dict(),\n",
    "            'critic1_optimizer_state_dict': agent.critic_1.optimizer.state_dict(),\n",
    "            'critic2_optimizer_state_dict': agent.critic_2.optimizer.state_dict(),\n",
    "            }, \"models/unity\" + str(i) + \"_steps_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"models/unity999_steps_weights.pt\")\n",
    "agent.actor.load_state_dict(ckpt['actor_state_dict'])\n",
    "agent.actor.optimizer.load_state_dict(ckpt['actor_optimizer_state_dict'])\n",
    "agent.critic_1.load_state_dict(ckpt['critic1_state_dict'])\n",
    "agent.critic_2.load_state_dict(ckpt['critic2_state_dict'])\n",
    "agent.critic_1.optimizer.load_state_dict(ckpt['critic1_optimizer_state_dict'])\n",
    "agent.critic_2.optimizer.load_state_dict(ckpt['critic2_optimizer_state_dict'])\n",
    "agent.critic_1_target.load_state_dict(ckpt['target_critic1_state_dict'])\n",
    "agent.critic_2_target.load_state_dict(ckpt['target_critic2_state_dict'])\n",
    "agent.actor.train()\n",
    "agent.critic_1.train()\n",
    "agent.critic_2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX (Unity format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset() \n",
    "observation = torch.tensor(np.array([observation]), dtype=torch.float).to(device)\n",
    "\n",
    "class WrapperNet(torch.nn.Module):\n",
    "    def __init__(self, actor, max_action):\n",
    "        \"\"\"\n",
    "        Wraps the VisualQNetwork adding extra constants and dummy mask inputs\n",
    "        required by runtime inference with Sentis.\n",
    "\n",
    "        For environment continuous actions outputs would need to add them\n",
    "        similarly to how discrete action outputs work, both in the wrapper\n",
    "        and in the ONNX output_names / dynamic_axes.\n",
    "        \"\"\"\n",
    "        super(WrapperNet, self).__init__()\n",
    "        self.qnet = actor\n",
    "        self.max_action = max_action\n",
    "\n",
    "        # version_number\n",
    "        #   MLAgents1_0 = 2   (not covered by this example)\n",
    "        #   MLAgents2_0 = 3\n",
    "        version_number = torch.Tensor([3])\n",
    "        self.version_number = nn.Parameter(version_number, requires_grad=False)\n",
    "\n",
    "        # memory_size\n",
    "        # TODO: document case where memory is not zero.\n",
    "        memory_size = torch.Tensor([0])\n",
    "        self.memory_size = nn.Parameter(memory_size, requires_grad=False)\n",
    "\n",
    "        # discrete_action_output_shape\n",
    "        self.continuous_shape = nn.Parameter(torch.tensor([max_action]), requires_grad=False)\n",
    "\n",
    "\n",
    "    # if you have discrete actions ML-agents expects corresponding a mask\n",
    "    # tensor with the same shape to exist as input\n",
    "    def forward(self, obs: torch.tensor):\n",
    "        loc, scale_log = self.qnet(obs)\n",
    "        scale = scale_log.exp()\n",
    "        dist = Normal(loc, scale)\n",
    "\n",
    "        sample = dist.sample()\n",
    "\n",
    "        action = torch.tanh(sample)*torch.tensor(max_action).to(device)\n",
    "\n",
    "        \n",
    "        return [action], self.continuous_shape, self.version_number, self.memory_size\n",
    "    \n",
    "model = WrapperNet(agent.actor, env.action_space.shape)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    observation,\n",
    "    'UnityTest.onnx',\n",
    "    opset_version=11,\n",
    "    # input_names must correspond to the WrapperNet forward parameters\n",
    "    # obs will be obs_0, obs_1, etc.\n",
    "    input_names=[\"obs_0\"],\n",
    "    # output_names must correspond to the return tuple of the WrapperNet\n",
    "    # forward function.\n",
    "    output_names=[\"continuous_actions\", \"continuous_action_output_shape\",\n",
    "                  \"version_number\", \"memory_size\"],\n",
    "    # All inputs and outputs should have their 0th dimension be designated\n",
    "    # as 'batch'\n",
    "    dynamic_axes={'obs_0': {0: 'batch'},\n",
    "                  'continuous_actions': {0: 'batch'},\n",
    "                  'continuous_action_output_shape': {0: 'batch'}\n",
    "                 }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
